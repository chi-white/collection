{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this python program, we calculate the I-score for combination of the all various variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the binary dataset\n",
    "\n",
    "# data = pd.read_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\GMC_deal.csv\").drop(columns='Unnamed: 0')\n",
    "# orig_data = pd.read_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\gmc-training.csv\").drop(columns='Unnamed: 0')\n",
    "\n",
    "# data['x3'] = orig_data['NumberOfTime30-59DaysPastDueNotWorse']\n",
    "# data['x8'] = orig_data['NumberOfTimes90DaysLate']\n",
    "# data['x9'] = orig_data['NumberRealEstateLoansOrLines'] \n",
    "# data['x10'] = orig_data['NumberOfTime60-89DaysPastDueNotWorse']\n",
    "# data['x11'] = orig_data['NumberOfDependents']\n",
    "\n",
    "# #first step, I want to change this data into discrete type. So, I will using the guide provided by YC to deal with.\n",
    "# #x3, x8, x9, x10, x11 are binary change, value = 0 if value==0 else 1.\n",
    "# v1 = ['x3', 'x8', 'x9', 'x10', 'x11']\n",
    "# for variable in v1 :\n",
    "#     data[variable] = pd.np.where(data[variable]==0, 0, 1)\n",
    "\n",
    "# #x1, x2, x4, x5, x7 are binary change value = 0 if value < median else 1.\n",
    "# v2 = ['x1', 'x2', 'x4', 'x5', 'x7']\n",
    "# for variable in v2 :\n",
    "#     median = data[variable].median()\n",
    "#     data[variable] = pd.np.where(data[variable]<=median, 0, 1)\n",
    "\n",
    "# data.to_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\binary_GMC_deal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\GMC\\\\binary_GMC_deal.csv\").drop(columns='Unnamed: 0')\n",
    "X = data.drop(columns='y')\n",
    "y = data['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up I-score function \n",
    "def I_score(X, y, influential) :\n",
    "    I = 0\n",
    "    test = 0\n",
    "    total_mean = np.mean(y)\n",
    "    data = X.copy()\n",
    "    data['y'] = y\n",
    "    m = len(influential)\n",
    "    values_set = itertools.product([0, 1], repeat=m)\n",
    "    for values in values_set :\n",
    "        table = data\n",
    "        for value, variable in zip(values, influential) :\n",
    "            table = table[table[variable]==value]\n",
    "        group_amount = len(table)\n",
    "        test  += group_amount\n",
    "        if  group_amount :\n",
    "            sub_mean = sum(table['y'])/group_amount\n",
    "            I += (group_amount**2)*((sub_mean-total_mean)**2)\n",
    "    I /= len(data)\n",
    "    return I\n",
    "\n",
    "#set up thin out function\n",
    "def retaining(X, y, influential) :\n",
    "    standard = I_score(X, y, influential)\n",
    "    while True :\n",
    "        thin_influential = 0\n",
    "        for se_influential in itertools.combinations(influential, len(influential)-1) :\n",
    "            I = I_score(X, y, se_influential)\n",
    "            if I > standard : \n",
    "                thin_influential = se_influential\n",
    "                standard = I\n",
    "        if thin_influential == 0 : return influential, standard\n",
    "        influential = thin_influential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate rank data\n",
    "\n",
    "m_col = ['m=0', 'm=1', 'm=2', 'm=3', 'm=4', 'm=5', 'm=6']\n",
    "value_col = ['I0', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6']\n",
    "total_mean = y_train.mean()\n",
    "total_table = pd.DataFrame()\n",
    "for m, name, value in zip(range(1, 7), m_col, value_col) :\n",
    "    name_list = []\n",
    "    value_list = []\n",
    "    for combination in itertools.combinations(X_train.columns, m) :\n",
    "            I = I_score(X_train, y_train, combination)\n",
    "            name_list.append(combination)\n",
    "            value_list.append(I)\n",
    "    temp_table = pd.DataFrame({m_col[m]:name_list, value_col[m]:value_list})\n",
    "    temp_table = temp_table.sort_values(value_col[m], ascending=False)\n",
    "    temp_table = temp_table.reset_index().drop(columns='index')\n",
    "    total_table = pd.concat([total_table, temp_table], axis=1, ignore_index=True)\n",
    "total_table.to_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\GMC\\\\I-score\\\\Iscore6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thin out the variable\n",
    "\n",
    "# for i in range(2, 7) :\n",
    "#     result = []    \n",
    "#     for influential, oI in zip(rank_data['m='+str(i)], rank_data['I'+str(i)]) :\n",
    "#         d = dict()\n",
    "#         if isinstance(influential, str) :\n",
    "#             thin, I = retaining(X_train, y_train, eval(influential))\n",
    "#             d['Originalindex'] = influential\n",
    "#             d['OriginaliScore'] = oI\n",
    "#             d['resulting'] = thin\n",
    "#             d['ResultingiScore'] = I\n",
    "#             result.append(d)\n",
    "#         else : \n",
    "#             pd.DataFrame(result).sort_values(\"resulting\").sort_values(\"ResultingiScore\", ascending=False).to_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\GMC\\\\I-score\\\\Iscore\"+str(i)+\"_thin.csv\")\n",
    "#             break\n",
    "#     pd.DataFrame(result).sort_values(\"resulting\").sort_values(\"ResultingiScore\", ascending=False).to_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\GMC\\\\I-score\\\\Iscore\"+str(i)+\"_thin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\king0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data AUC: 0.850\n",
      "train data Accuracy: 0.936\n",
      "train data Error rate: 0.064\n",
      "train data Precision: 0.584\n",
      "train data Recall: 0.155\n",
      "train data F1 score: 0.245\n",
      "test data AUC: 0.843\n",
      "test data Accuracy: 0.938\n",
      "test data Error rate: 0.062\n",
      "test data Precision: 0.579\n",
      "test data Recall: 0.168\n",
      "test data F1 score: 0.261\n",
      "m= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\king0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data AUC: 0.849\n",
      "train data Accuracy: 0.935\n",
      "train data Error rate: 0.065\n",
      "train data Precision: 0.575\n",
      "train data Recall: 0.151\n",
      "train data F1 score: 0.240\n",
      "test data AUC: 0.842\n",
      "test data Accuracy: 0.938\n",
      "test data Error rate: 0.062\n",
      "test data Precision: 0.582\n",
      "test data Recall: 0.169\n",
      "test data F1 score: 0.262\n",
      "m= 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\king0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data AUC: 0.847\n",
      "train data Accuracy: 0.935\n",
      "train data Error rate: 0.065\n",
      "train data Precision: 0.572\n",
      "train data Recall: 0.150\n",
      "train data F1 score: 0.238\n",
      "test data AUC: 0.841\n",
      "test data Accuracy: 0.938\n",
      "test data Error rate: 0.062\n",
      "test data Precision: 0.585\n",
      "test data Recall: 0.166\n",
      "test data F1 score: 0.258\n",
      "m= 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\king0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data AUC: 0.847\n",
      "train data Accuracy: 0.935\n",
      "train data Error rate: 0.065\n",
      "train data Precision: 0.573\n",
      "train data Recall: 0.150\n",
      "train data F1 score: 0.237\n",
      "test data AUC: 0.841\n",
      "test data Accuracy: 0.938\n",
      "test data Error rate: 0.062\n",
      "test data Precision: 0.574\n",
      "test data Recall: 0.166\n",
      "test data F1 score: 0.258\n",
      "m= 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\king0\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data AUC: 0.840\n",
      "train data Accuracy: 0.935\n",
      "train data Error rate: 0.065\n",
      "train data Precision: 0.566\n",
      "train data Recall: 0.134\n",
      "train data F1 score: 0.217\n",
      "test data AUC: 0.835\n",
      "test data Accuracy: 0.937\n",
      "test data Error rate: 0.063\n",
      "test data Precision: 0.570\n",
      "test data Recall: 0.150\n",
      "test data F1 score: 0.238\n"
     ]
    }
   ],
   "source": [
    "#make Logistic regression for retaining variables\n",
    "\n",
    "for i in range(2, 7) :\n",
    "    print('m=',i)\n",
    "    s = set()\n",
    "    thin_influential = pd.read_csv(\"C:\\\\Users\\\\king0\\\\OneDrive\\\\鄧惠文專題\\\\data\\\\GMC\\\\I-score\\\\Iscore\"+str(i)+\"_thin.csv\")['resulting']\n",
    "\n",
    "    for thin in thin_influential :\n",
    "        s.add(eval(thin))\n",
    "    newpara_train = pd.DataFrame()\n",
    "    newpara_test = pd.DataFrame()\n",
    "    for thin in s :\n",
    "        newpara_train[thin] = X_train[list(thin)].prod(axis=1)\n",
    "        newpara_test[thin] = X_test[list(thin)].prod(axis=1)\n",
    "    poly = PolynomialFeatures(3)\n",
    "    newpara_train = poly.fit_transform(newpara_train)\n",
    "    newpara_test = poly.fit_transform(newpara_test)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(newpara_train, y_train)\n",
    "\n",
    "    prob = model.predict_proba(newpara_train)[:, 1]\n",
    "    auc = roc_auc_score(y_train, prob)\n",
    "    print('train data AUC: %.3f' % auc)\n",
    "    y_pred = model.predict(newpara_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    print('train data Accuracy: %.3f' % acc)\n",
    "    print('train data Error rate: %.3f' % (1-acc))\n",
    "    precision = precision_score(y_train, y_pred)\n",
    "    print('train data Precision: %.3f' % precision)\n",
    "    recall = recall_score(y_train, y_pred)\n",
    "    print('train data Recall: %.3f' % recall)\n",
    "    f1 = f1_score(y_train, y_pred)\n",
    "    print('train data F1 score: %.3f' % f1)\n",
    "\n",
    "    prob = model.predict_proba(newpara_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, prob)\n",
    "    print('test data AUC: %.3f' % auc)\n",
    "    y_pred = model.predict(newpara_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print('test data Accuracy: %.3f' % acc)\n",
    "    print('test data Error rate: %.3f' % (1-acc))\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('test data Precision: %.3f' % precision)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print('test data Recall: %.3f' % recall)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print('test data F1 score: %.3f' % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1 col2\n",
      "2     1    a\n",
      "1     2    b\n",
      "0     3    c\n"
     ]
    }
   ],
   "source": [
    "# m= 2\n",
    "# train data AUC: 0.845\n",
    "# train data Accuracy: 0.934\n",
    "# train data Error rate: 0.066\n",
    "# train data Precision: 0.546\n",
    "# train data Recall: 0.152\n",
    "# train data F1 score: 0.238\n",
    "# test data AUC: 0.842\n",
    "# test data Accuracy: 0.938\n",
    "# test data Error rate: 0.062\n",
    "# test data Precision: 0.571\n",
    "# test data Recall: 0.171\n",
    "# test data F1 score: 0.263\n",
    "# m= 3\n",
    "# train data AUC: 0.844\n",
    "# train data Accuracy: 0.935\n",
    "# train data Error rate: 0.065\n",
    "# train data Precision: 0.559\n",
    "# train data Recall: 0.147\n",
    "# train data F1 score: 0.232\n",
    "# test data AUC: 0.841\n",
    "# test data Accuracy: 0.937\n",
    "# test data Error rate: 0.063\n",
    "# test data Precision: 0.571\n",
    "# test data Recall: 0.165\n",
    "# test data F1 score: 0.256\n",
    "# m= 4\n",
    "# train data AUC: 0.844\n",
    "# train data Accuracy: 0.935\n",
    "# train data Error rate: 0.065\n",
    "# train data Precision: 0.558\n",
    "# train data Recall: 0.150\n",
    "# train data F1 score: 0.236\n",
    "# test data AUC: 0.841\n",
    "# test data Accuracy: 0.937\n",
    "# test data Error rate: 0.063\n",
    "# test data Precision: 0.558\n",
    "# test data Recall: 0.166\n",
    "# test data F1 score: 0.256\n",
    "# m= 5\n",
    "# train data AUC: 0.843\n",
    "# train data Accuracy: 0.935\n",
    "# train data Error rate: 0.065\n",
    "# train data Precision: 0.560\n",
    "# train data Recall: 0.147\n",
    "# train data F1 score: 0.233\n",
    "# test data AUC: 0.841\n",
    "# test data Accuracy: 0.937\n",
    "# test data Error rate: 0.063\n",
    "# test data Precision: 0.568\n",
    "# test data Recall: 0.165\n",
    "# test data F1 score: 0.255\n",
    "# m= 6\n",
    "# train data AUC: 0.839\n",
    "# train data Accuracy: 0.935\n",
    "# train data Error rate: 0.065\n",
    "# train data Precision: 0.562\n",
    "# train data Recall: 0.127\n",
    "# train data F1 score: 0.207\n",
    "# test data AUC: 0.835\n",
    "# test data Accuracy: 0.937\n",
    "# test data Error rate: 0.063\n",
    "# test data Precision: 0.576\n",
    "# test data Recall: 0.144\n",
    "# test data F1 score: 0.230"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
